{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9260ac5a-bfb1-4a39-b182-082c9de52e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/outputs/processed/emails_merged.processed.csv\n",
      "[Insight 1] Spam category shares:\n",
      " category\n",
      "Financial      0.262\n",
      "Promotional    0.243\n",
      "Action         0.167\n",
      "Urgency        0.069\n",
      "Technical      0.048\n",
      "Other          0.210\n",
      "Name: count, dtype: float64\n",
      "[Insight 2] Top spam-related words:\n",
      "  subject (spam 67.79% vs ham 47.69%)\n",
      "  the (spam 61.45% vs ham 53.59%)\n",
      "  you (spam 59.86% vs ham 53.26%)\n",
      "  your (spam 57.98% vs ham 29.86%)\n",
      "  and (spam 54.16% vs ham 46.94%)\n",
      "[Insight 3] Overlap between top-15 ham & spam tokens: 10\n",
      "[Insight 4] Correlation(word_count, digit_density) — harmless=0.215, spam=-0.185.\n",
      "Saved all visualizations to: /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/outputs/analyzed\n"
     ]
    }
   ],
   "source": [
    "# save as: /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/data_processing/analyze_emails.py\n",
    "# Runs in Jupyter or as a standalone script.\n",
    "# Produces 4 detailed visualizations saved under: Assignment_2/outputs/analyzed/\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "# Use non-interactive backend if not in Jupyter\n",
    "if not any(k.startswith(\"JPY_PARENT_PID\") for k in os.environ):\n",
    "    matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "try:\n",
    "    here = Path(__file__).resolve().parent   # script mode\n",
    "except NameError:\n",
    "    here = Path.cwd().resolve()              # Jupyter mode\n",
    "\n",
    "# Try a few likely bases (this dir, parents, and with/without 'Assignment_2')\n",
    "candidates = [here, here.parent, here.parent.parent]\n",
    "candidates += [b / \"Assignment_2\" for b in list(candidates)]\n",
    "\n",
    "DATA = None\n",
    "for base in candidates:\n",
    "    probe = base / \"outputs\" / \"processed\" / \"emails_merged.processed.csv\"\n",
    "    if probe.exists():\n",
    "        ROOT = base\n",
    "        DATA = probe\n",
    "        break\n",
    "\n",
    "if DATA is None:\n",
    "    # Fallback to your explicit project path\n",
    "    ROOT = Path(\"/Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2\")\n",
    "    DATA = ROOT / \"outputs\" / \"processed\" / \"emails_merged.processed.csv\"\n",
    "\n",
    "OUTDIR = ROOT / \"outputs\" / \"analyzed\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Reading: {DATA}\")\n",
    "\n",
    "# -------------------- Load ---------------------\n",
    "df = pd.read_csv(DATA)\n",
    "assert {\"text\", \"label\"}.issubset(df.columns), \"emails_merged.processed.csv must have columns: text,label\"\n",
    "\n",
    "# -------------------- Feature engineering -------------\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"char_len\"] = df[\"text\"].str.len()\n",
    "df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "df[\"has_digit\"] = df[\"text\"].str.contains(r\"\\d\")\n",
    "df[\"digit_count\"] = df[\"text\"].str.count(r\"\\d\")\n",
    "df[\"digit_density\"] = df[\"digit_count\"] / df[\"char_len\"].replace(0, np.nan)\n",
    "\n",
    "# =============================================================================\n",
    "# ==================== Insight 1: Spam keyword categories (PIE) ====================\n",
    "# Categorize spam messages by simple keyword rules\n",
    "categories = {\n",
    "    \"Financial\":   [r\"\\b(pay|cash|credit|loan|account|bank|invoice|usd|dollar|price|cost)\\b\"],\n",
    "    \"Promotional\": [r\"\\b(free|win|offer|promo|deal|discount|sale|subscribe|prize|bonus)\\b\"],\n",
    "    \"Action\":      [r\"\\b(click|confirm|verify|download|open|reply|call|visit)\\b\"],\n",
    "    \"Urgency\":     [r\"\\b(urgent|now|immediately|act|limited|expires|last chance)\\b\"],\n",
    "    \"Technical\":   [r\"\\b(http|www|link|email|password|unsubscribe)\\b\"],\n",
    "}\n",
    "spam_df = df[df[\"label\"] == 1].copy()\n",
    "\n",
    "def assign_category(text: str) -> str:\n",
    "    for cat, patterns in categories.items():\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, text):\n",
    "                return cat\n",
    "    return \"Other\"\n",
    "\n",
    "spam_df[\"category\"] = spam_df[\"text\"].str.lower().map(assign_category)\n",
    "cat_counts = spam_df[\"category\"].value_counts().reindex(\n",
    "    [\"Financial\",\"Promotional\",\"Action\",\"Urgency\",\"Technical\",\"Other\"]\n",
    ").fillna(0)\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8))\n",
    "plt.pie(cat_counts.values, labels=cat_counts.index, autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Spam Keyword Categories Distribution\")\n",
    "plt.tight_layout()\n",
    "fig1_path = OUTDIR / \"keyword_categories_pie.png\"\n",
    "plt.savefig(fig1_path); plt.close(fig1)\n",
    "\n",
    "print(\"[Insight 1] Spam category shares:\\n\", (cat_counts / max(cat_counts.sum(),1)).round(3))\n",
    "\n",
    "# =============================================================================\n",
    "# ========== Insight 2: Keyword presence rates (top 15 by spam rate) (BAR, optimized) ==========\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"\\b[a-z0-9]{3,}\\b\",  # ignore very-short tokens\n",
    "    max_features=2000,                  # speed guard\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Use integer indices (not boolean Series) for sparse row selection\n",
    "spam_idx = np.flatnonzero(df[\"label\"].to_numpy() == 1)\n",
    "ham_idx  = np.flatnonzero(df[\"label\"].to_numpy() == 0)\n",
    "\n",
    "# Presence (True if token appears at least once in a message)\n",
    "spam_means = (X[spam_idx] > 0).mean(axis=0).A1\n",
    "ham_means  = (X[ham_idx]  > 0).mean(axis=0).A1\n",
    "\n",
    "# Top 15 by spam presence rate\n",
    "idx = np.argsort(spam_means)[::-1][:15]\n",
    "top_words = vocab[idx]\n",
    "s_vals = spam_means[idx]\n",
    "h_vals = ham_means[idx]\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(top_words))\n",
    "width = 0.42\n",
    "fig2 = plt.figure(figsize=(12,7))\n",
    "plt.bar(x - width/2, s_vals, width, label=\"Spam\", color=\"#ff7f7f\")\n",
    "plt.bar(x + width/2, h_vals, width, label=\"Ham\",  color=\"#b59b3d\")\n",
    "plt.xticks(x, top_words, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Fraction of messages containing word\")\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.title(\"Keyword Presence Rates (Top 15 by Spam Rate)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig2_path = OUTDIR / \"keyword_presence_rates.png\"\n",
    "plt.savefig(fig2_path); plt.close(fig2)\n",
    "\n",
    "# Textual insight\n",
    "top5 = [f\"{w} (spam {s:.2%} vs ham {h:.2%})\" for w, s, h in zip(top_words[:5], s_vals[:5], h_vals[:5])]\n",
    "print(\"[Insight 2] Top spam-related words:\\n  \" + \"\\n  \".join(top5))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ===== Insight 3: Top-15 tokens by class (dual horizontal BARs, side-by-side) =====\n",
    "tokens = texts.str.findall(token_re)\n",
    "spam_tok = tokens[df[\"label\"] == 1].explode().value_counts().head(15)\n",
    "ham_tok  = tokens[df[\"label\"] == 0].explode().value_counts().head(15)\n",
    "\n",
    "fig3, ax = plt.subplots(1, 2, figsize=(16,6), sharey=False)\n",
    "ax[0].barh(ham_tok.index[::-1], ham_tok.values[::-1], color=\"#86c5da\")\n",
    "ax[0].set_title(\"Top 15 Tokens in Ham Emails\"); ax[0].set_xlabel(\"Frequency\")\n",
    "\n",
    "ax[1].barh(spam_tok.index[::-1], spam_tok.values[::-1], color=\"#e78b8b\")\n",
    "ax[1].set_title(\"Top 15 Tokens in Spam Emails\"); ax[1].set_xlabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig3_path = OUTDIR / \"token_frequency_comparison.png\"\n",
    "plt.savefig(fig3_path); plt.close(fig3)\n",
    "\n",
    "print(\"[Insight 3] Overlap between top-15 ham & spam tokens:\",\n",
    "      len(set(ham_tok.index).intersection(set(spam_tok.index))))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Insight 4 (SCATTER): Word count vs digit density — with trend lines & correlations\n",
    "# =============================================================================\n",
    "mask0 = (df[\"label\"] == 0)\n",
    "mask1 = (df[\"label\"] == 1)\n",
    "\n",
    "x0, y0 = df.loc[mask0, \"word_count\"].values, df.loc[mask0, \"digit_density\"].values\n",
    "x1, y1 = df.loc[mask1, \"word_count\"].values, df.loc[mask1, \"digit_density\"].values\n",
    "\n",
    "fig4 = plt.figure()\n",
    "plt.scatter(x0, y0, alpha=0.3, label=\"harmless (0)\", s=15)\n",
    "plt.scatter(x1, y1, alpha=0.3, label=\"spam (1)\", s=15, marker=\"^\")\n",
    "\n",
    "# add simple linear fits (no extra deps)\n",
    "if np.isfinite(x0).all() and np.isfinite(y0).all() and len(x0) > 1:\n",
    "    m0, b0 = np.polyfit(x0[np.isfinite(x0) & np.isfinite(y0)],\n",
    "                        y0[np.isfinite(x0) & np.isfinite(y0)], 1)\n",
    "    xgrid0 = np.linspace(np.nanmin(x0), np.nanmax(x0), 100)\n",
    "    plt.plot(xgrid0, m0*xgrid0 + b0, linestyle=\"--\", label=\"trend (0)\")\n",
    "\n",
    "if np.isfinite(x1).all() and np.isfinite(y1).all() and len(x1) > 1:\n",
    "    m1, b1 = np.polyfit(x1[np.isfinite(x1) & np.isfinite(y1)],\n",
    "                        y1[np.isfinite(x1) & np.isfinite(y1)], 1)\n",
    "    xgrid1 = np.linspace(np.nanmin(x1), np.nanmax(x1), 100)\n",
    "    plt.plot(xgrid1, m1*xgrid1 + b1, linestyle=\":\", label=\"trend (1)\")\n",
    "\n",
    "plt.title(\"Word Count vs Digit Density (with linear trends)\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Digit Density (digits / characters)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "fig4_path = OUTDIR / \"scatter_wordcount_vs_digitdensity.png\"\n",
    "plt.savefig(fig4_path)\n",
    "plt.close(fig4)\n",
    "\n",
    "# correlations\n",
    "def safe_corr(a, b):\n",
    "    a = pd.Series(a).astype(float)\n",
    "    b = pd.Series(b).astype(float)\n",
    "    a = a.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    b = b.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    n = min(len(a), len(b))\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    return np.corrcoef(a[:n], b[:n])[0,1]\n",
    "\n",
    "r0 = safe_corr(x0, y0)\n",
    "r1 = safe_corr(x1, y1)\n",
    "print(f\"[Insight 4] Correlation(word_count, digit_density) — harmless={r0:.3f}, spam={r1:.3f}.\")\n",
    "\n",
    "# -------------------- Save a small README -----------------\n",
    "with open(OUTDIR / \"README.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        \"Figures generated by analyze_emails.py\\n\"\n",
    "        f\"- {fig1_path.name}: Label distribution with counts & % (bar)\\n\"\n",
    "        f\"- {fig2_path.name}: Top-10 words — class-wise proportions, size~freq (scatter)\\n\"\n",
    "        f\"- {fig3_path.name}: CDF of char length (overall & by class) + quantiles (line)\\n\"\n",
    "        f\"- {fig4_path.name}: Word count vs digit density with trend lines (scatter)\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"Saved all visualizations to: {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd9784-8c89-4aa3-9b14-0ff3ed840adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a584b-42de-481f-9e97-8a953f742516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca121c-e9d5-463c-aa84-8b5270938d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
