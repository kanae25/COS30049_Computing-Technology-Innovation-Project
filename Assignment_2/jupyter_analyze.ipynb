{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb6b7eb-72d6-4caa-8ea6-97c97a4746e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ROOT        : /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2\n",
      "[INFO] INPUT CSV   : /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/outputs/processed/emails_merged.processed.csv\n",
      "[INFO] ANALYZE DIR : /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/outputs/analyze\n",
      "Running EDA on DataFrame with shape: (10715, 2)\n",
      "Saved unigram plots.\n",
      "Saved bigram plots.\n",
      "Saved chi2 discriminative terms.\n",
      "Saved keyword presence rates and plot.\n",
      "Saved token lift CSVs.\n",
      "Saved quick summary.\n",
      "\n",
      "All files written to /Users/datschef/Documents/GitHub/cos30049_spam_detection/Assignment_2/outputs/analyze\n",
      " - chi2_discriminative_terms.csv\n",
      " - keyword_presence_rates.csv\n",
      " - keyword_presence_rates_fixed.png\n",
      " - quick_insights.txt\n",
      " - token_lift_top_ham.csv\n",
      " - token_lift_top_spam.csv\n",
      " - top25_bigrams_ham.png\n",
      " - top25_bigrams_spam.png\n",
      " - top25_unigrams_ham.png\n",
      " - top25_unigrams_spam.png\n"
     ]
    }
   ],
   "source": [
    "# data_analyze/data_analyze.py\n",
    "# Runs EDA and writes ALL outputs to: outputs/analyze/\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os, re, numpy as np, pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")                 # headless image writing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# ----------------------------\n",
    "# Robust project path handling\n",
    "# ----------------------------\n",
    "def _resolve_root() -> Path:\n",
    "    \"\"\"\n",
    "    If run as a script: ROOT = parent of data_analyze/.\n",
    "    If run from Jupyter: walk up from CWD until we find both 'outputs' and 'datasets'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parents[1]\n",
    "    except NameError:\n",
    "        cur = Path.cwd().resolve()\n",
    "        for _ in range(6):\n",
    "            if (cur / \"datasets\").exists() and (cur / \"outputs\").exists():\n",
    "                return cur\n",
    "            if cur.parent == cur:\n",
    "                break\n",
    "            cur = cur.parent\n",
    "        raise RuntimeError(\n",
    "            \"Couldn't locate project root. Open the notebook from 'Assignment_2' \"\n",
    "            \"so that 'datasets/' and 'outputs/' are visible.\"\n",
    "        )\n",
    "\n",
    "ROOT = _resolve_root()\n",
    "PROCESSED_DIR = ROOT / \"outputs\" / \"processed\"\n",
    "ANALYZE_DIR   = ROOT / \"outputs\" / \"analyze\"    # <â€” all EDA outputs go here\n",
    "ANALYZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_INPUT = PROCESSED_DIR / \"emails_merged.processed.csv\"\n",
    "\n",
    "print(\"[INFO] ROOT        :\", ROOT)\n",
    "print(\"[INFO] INPUT CSV   :\", DEFAULT_INPUT)\n",
    "print(\"[INFO] ANALYZE DIR :\", ANALYZE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Small helpers\n",
    "# ----------------------------\n",
    "def basic_length_stats(texts: pd.Series) -> pd.DataFrame:\n",
    "    s = texts.fillna(\"\").astype(str)\n",
    "    num_chars = s.str.len()\n",
    "    num_words = s.str.split().apply(len)\n",
    "    avg_word_len = np.where(num_words > 0, num_chars / num_words, 0.0)\n",
    "    return pd.DataFrame({\n",
    "        \"num_chars\": num_chars,\n",
    "        \"num_words\": num_words,\n",
    "        \"avg_word_len\": avg_word_len,\n",
    "    }, index=texts.index)\n",
    "\n",
    "def _ensure_df(df: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    if df is not None:\n",
    "        return df\n",
    "    if not DEFAULT_INPUT.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected processed file at {DEFAULT_INPUT}.\\n\"\n",
    "            \"Run the processing step first to create it.\"\n",
    "        )\n",
    "    return pd.read_csv(DEFAULT_INPUT)\n",
    "\n",
    "# ----------------------------\n",
    "# Main EDA routine\n",
    "# ----------------------------\n",
    "def analyze(df: pd.DataFrame | None = None, out_dir: Path | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    Perform EDA and write all artifacts to out_dir (defaults to outputs/analyze).\n",
    "    If df is None, loads DEFAULT_INPUT.\n",
    "    Returns the output directory path.\n",
    "    \"\"\"\n",
    "    out = Path(out_dir) if out_dir else ANALYZE_DIR\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = _ensure_df(df)\n",
    "    if not {\"text\", \"label\"}.issubset(df.columns):\n",
    "        raise ValueError(\"Input DataFrame must have columns: 'text' and 'label'.\")\n",
    "\n",
    "    print(\"Running EDA on DataFrame with shape:\", df.shape)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1) Top unigrams by class\n",
    "    # ------------------------------\n",
    "    vectorizer_uni = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "    X_all = vectorizer_uni.fit_transform(df[\"text\"])\n",
    "    vocab = np.array(vectorizer_uni.get_feature_names_out())\n",
    "\n",
    "    X_ham = X_all[df[\"label\"].values == 0]\n",
    "    X_spam = X_all[df[\"label\"].values == 1]\n",
    "\n",
    "    ham_counts = np.asarray(X_ham.sum(axis=0)).ravel()\n",
    "    spam_counts = np.asarray(X_spam.sum(axis=0)).ravel()\n",
    "\n",
    "    top_ham = pd.DataFrame({\"token\": vocab, \"count\": ham_counts}).sort_values(\"count\", ascending=False).head(25)\n",
    "    top_spam = pd.DataFrame({\"token\": vocab, \"count\": spam_counts}).sort_values(\"count\", ascending=False).head(25)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(top_ham[\"token\"], top_ham[\"count\"])\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.title(\"Top 25 Unigrams - Ham\")\n",
    "    plt.xlabel(\"token\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout(); plt.savefig(out / \"top25_unigrams_ham.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(top_spam[\"token\"], top_spam[\"count\"])\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.title(\"Top 25 Unigrams - Spam\")\n",
    "    plt.xlabel(\"token\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout(); plt.savefig(out / \"top25_unigrams_spam.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    print(\"Saved unigram plots.\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2) Top bigrams by class\n",
    "    # ------------------------------\n",
    "    vectorizer_bi = CountVectorizer(stop_words=\"english\", min_df=5, ngram_range=(2,2))\n",
    "    X_all_bi = vectorizer_bi.fit_transform(df[\"text\"])\n",
    "    vocab_bi = np.array(vectorizer_bi.get_feature_names_out())\n",
    "\n",
    "    X_ham_bi = X_all_bi[df[\"label\"].values == 0]\n",
    "    X_spam_bi = X_all_bi[df[\"label\"].values == 1]\n",
    "\n",
    "    ham_counts_bi = np.asarray(X_ham_bi.sum(axis=0)).ravel()\n",
    "    spam_counts_bi = np.asarray(X_spam_bi.sum(axis=0)).ravel()\n",
    "\n",
    "    top_ham_bi = pd.DataFrame({\"bigram\": vocab_bi, \"count\": ham_counts_bi}).sort_values(\"count\", ascending=False).head(25)\n",
    "    top_spam_bi = pd.DataFrame({\"bigram\": vocab_bi, \"count\": spam_counts_bi}).sort_values(\"count\", ascending=False).head(25)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(top_ham_bi[\"bigram\"], top_ham_bi[\"count\"])\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.title(\"Top 25 Bigrams - Ham\")\n",
    "    plt.xlabel(\"bigram\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout(); plt.savefig(out / \"top25_bigrams_ham.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(top_spam_bi[\"bigram\"], top_spam_bi[\"count\"])\n",
    "    plt.xticks(rotation=75, ha=\"right\")\n",
    "    plt.title(\"Top 25 Bigrams - Spam\")\n",
    "    plt.xlabel(\"bigram\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout(); plt.savefig(out / \"top25_bigrams_spam.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    print(\"Saved bigram plots.\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3) TF-IDF + Chi-square discriminative terms\n",
    "    # ------------------------------\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", min_df=5)\n",
    "    X_tfidf = tfidf.fit_transform(df[\"text\"])\n",
    "    y = df[\"label\"].values\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        chi2_scores, pvals = chi2(X_tfidf, y)\n",
    "        X_spam_tfidf = X_tfidf[y == 1]\n",
    "        X_ham_tfidf  = X_tfidf[y == 0]\n",
    "        mean_spam = np.asarray(X_spam_tfidf.mean(axis=0)).ravel()\n",
    "        mean_ham  = np.asarray(X_ham_tfidf.mean(axis=0)).ravel()\n",
    "\n",
    "        df_chi = pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"chi2\": chi2_scores,\n",
    "            \"pval\": pvals,\n",
    "            \"mean_spam\": mean_spam,\n",
    "            \"mean_ham\": mean_ham,\n",
    "            \"spam_minus_ham\": mean_spam - mean_ham\n",
    "        }).sort_values(\"chi2\", ascending=False)\n",
    "\n",
    "        df_chi.to_csv(out / \"chi2_discriminative_terms.csv\", index=False)\n",
    "        print(\"Saved chi2 discriminative terms.\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: chi2 step failed:\", e)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4) Keyword presence rates\n",
    "    # ------------------------------\n",
    "    keywords = [\n",
    "        \"free\", \"win\", \"winner\", \"prize\", \"money\", \"cash\", \"click\", \"offer\", \"buy\", \"discount\",\n",
    "        \"urgent\", \"account\", \"verify\", \"http\", \"www\", \"unsubscribe\", \"viagra\", \"loan\", \"credit\", \"bitcoin\"\n",
    "    ]\n",
    "\n",
    "    def keyword_presence_rate(texts: pd.Series, kw: str) -> float:\n",
    "        return texts.str.contains(rf\"\\b{re.escape(kw)}\\b\", regex=True, case=False).mean()\n",
    "\n",
    "    rows = []\n",
    "    for kw in keywords:\n",
    "        rows.append({\n",
    "            \"keyword\": kw,\n",
    "            \"rate_spam\": keyword_presence_rate(df.loc[df[\"label\"]==1, \"text\"], kw),\n",
    "            \"rate_ham\":  keyword_presence_rate(df.loc[df[\"label\"]==0, \"text\"], kw)\n",
    "        })\n",
    "\n",
    "    kw_df = pd.DataFrame(rows).sort_values(\"rate_spam\", ascending=False)\n",
    "    kw_df.to_csv(out / \"keyword_presence_rates.csv\", index=False)\n",
    "\n",
    "    # plot (top 15 by spam rate)\n",
    "    top15 = kw_df.head(15)\n",
    "    x = np.arange(len(top15))\n",
    "    width = 0.35\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(x - width/2, top15[\"rate_spam\"], width, label=\"spam\")\n",
    "    plt.bar(x + width/2, top15[\"rate_ham\"], width, label=\"ham\")\n",
    "    plt.xticks(x, top15[\"keyword\"], rotation=75, ha=\"right\")\n",
    "    plt.title(\"Keyword presence rates (top 15 by spam rate)\")\n",
    "    plt.xlabel(\"keyword\"); plt.ylabel(\"fraction of messages\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out / \"keyword_presence_rates_fixed.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    print(\"Saved keyword presence rates and plot.\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 5) Token lift (log-ratio spam vs ham)\n",
    "    # ------------------------------\n",
    "    # reuse unigram totals\n",
    "    ham_total = ham_counts.sum()\n",
    "    spam_total = spam_counts.sum()\n",
    "    ham_pct  = (ham_counts  + 1) / (ham_total  + len(ham_counts))\n",
    "    spam_pct = (spam_counts + 1) / (spam_total + len(spam_counts))\n",
    "\n",
    "    log_ratio = np.log(spam_pct / ham_pct)\n",
    "    lift_df = pd.DataFrame({\"token\": vocab, \"log_ratio\": log_ratio, \"spam_pct\": spam_pct, \"ham_pct\": ham_pct})\n",
    "\n",
    "    top_spam_skew = lift_df.sort_values(\"log_ratio\", ascending=False).head(30)\n",
    "    top_ham_skew  = lift_df.sort_values(\"log_ratio\", ascending=True).head(30)\n",
    "\n",
    "    (out / \"token_lift_top_spam.csv\").write_text(\"\")  # ensure file presence on some FS\n",
    "    top_spam_skew.to_csv(out / \"token_lift_top_spam.csv\", index=False)\n",
    "    top_ham_skew.to_csv(out / \"token_lift_top_ham.csv\", index=False)\n",
    "\n",
    "    print(\"Saved token lift CSVs.\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 6) Quick text summary\n",
    "    # ------------------------------\n",
    "    n_total = len(df)\n",
    "    n_spam  = int(df[\"label\"].sum())\n",
    "    n_ham   = n_total - n_spam\n",
    "\n",
    "    len_means = (\n",
    "        pd.concat([df[[\"label\"]], basic_length_stats(df[\"text\"])], axis=1)\n",
    "          .groupby(\"label\")[[\"num_chars\",\"num_words\",\"avg_word_len\"]]\n",
    "          .mean()\n",
    "    )\n",
    "\n",
    "    summary_lines = []\n",
    "    summary_lines.append(f\"Total messages: {n_total:,}\")\n",
    "    summary_lines.append(f\"Ham (0): {n_ham:,}  |  Spam (1): {n_spam:,}  |  Spam rate: {n_spam/n_total:.2%}\")\n",
    "    summary_lines.append(\"\")\n",
    "    summary_lines.append(\"Average lengths by label:\")\n",
    "    for lbl, row in len_means.iterrows():\n",
    "        summary_lines.append(f\"  Label {lbl}: num_chars={row['num_chars']:.1f}, num_words={row['num_words']:.1f}, avg_word_len={row['avg_word_len']:.2f}\")\n",
    "    summary_lines.append(\"\")\n",
    "    summary_lines.append(\"Top spam-skewed tokens (log-ratio):\")\n",
    "    summary_lines.append(\", \".join(top_spam_skew.head(15)[\"token\"].tolist()))\n",
    "    summary_lines.append(\"\")\n",
    "    summary_lines.append(\"Top ham-skewed tokens (log-ratio):\")\n",
    "    summary_lines.append(\", \".join(top_ham_skew.head(15)[\"token\"].tolist()))\n",
    "\n",
    "    with open(out / \"quick_insights.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(summary_lines))\n",
    "    print(\"Saved quick summary.\")\n",
    "\n",
    "    # List produced files\n",
    "    print(\"\\nAll files written to\", out.resolve())\n",
    "    for p in sorted(out.iterdir()):\n",
    "        print(\" -\", p.name)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# CLI entrypoint\n",
    "# ----------------------------\n",
    "def main():\n",
    "    analyze()   # loads DEFAULT_INPUT and writes to ANALYZE_DIR\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e57952-3f90-49d4-96f5-378502ca16ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
