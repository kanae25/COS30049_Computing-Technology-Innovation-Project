{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6712a71-f79c-4f65-8500-0b77f6add770",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ---------- Paths ----------\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m SCRIPT_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m     10\u001b[0m ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(SCRIPT_DIR)              \u001b[38;5;66;03m# ASSIGNMENT_2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m DATA_IN  \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# data_processor.py\n",
    "# Outputs are ALL: columns -> text,label  (0 = harmless, 1 = spam)\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, unicodedata, string\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Paths ----------\n",
    "SCRIPT_DIR = os.path.dirname(__file__)\n",
    "ROOT = os.path.dirname(SCRIPT_DIR)              # ASSIGNMENT_2\n",
    "DATA_IN  = os.path.join(ROOT, \"datasets\")\n",
    "DATA_OUT = os.path.join(ROOT, \"outputs\", \"processed\")\n",
    "os.makedirs(DATA_OUT, exist_ok=True)\n",
    "\n",
    "FILES = {\n",
    "    \"email_spam\": os.path.join(DATA_IN, \"email_spam.csv\"),\n",
    "    \"emails\":     os.path.join(DATA_IN, \"emails.csv\"),\n",
    "    \"text_spam\":  os.path.join(DATA_IN, \"text_spam.csv\"),\n",
    "}\n",
    "\n",
    "# ---------- Cleaning helpers ----------\n",
    "URL_RE   = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "HTML_RE  = re.compile(r\"<[^>]+>\")\n",
    "PUNCT_TABLE = str.maketrans({c: \" \" for c in string.punctuation + \"“”‘’—–…•·”’“\"})\n",
    "\n",
    "def _ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.replace(\"\\u00A0\", \" \")\n",
    "    t = HTML_RE.sub(\" \", t)                    # drop HTML tags\n",
    "    t = URL_RE.sub(\" \", t)                     # drop full URLs\n",
    "    t = EMAIL_RE.sub(\" \", t)                   # drop email addresses\n",
    "    t = unicodedata.normalize(\"NFKD\", t).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    t = t.lower().translate(PUNCT_TABLE)       # remove punctuation, keep digits\n",
    "    # NOTE: no re.sub(r\"\\d+\", \" \", t) here — we keep numbers\n",
    "    return _ws(t)\n",
    "\n",
    "\n",
    "def drop_empty_dupes(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    # Ensure it's a string Series (guard against duplicate-named columns)\n",
    "    col_data = df[col]\n",
    "    if isinstance(col_data, pd.DataFrame):\n",
    "        col_data = col_data.iloc[:, 0]  # take the first 'text' if duplicates exist\n",
    "    s = col_data.astype(str)\n",
    "\n",
    "    df = df.assign(**{col: s})\n",
    "    df = df[df[col].str.strip().astype(bool)]\n",
    "    return df.drop_duplicates(subset=[col])\n",
    "\n",
    "def to_2col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Build the 2-column frame explicitly to avoid name collisions\n",
    "    text_series = df[\"clean_text\"] if \"clean_text\" in df.columns else df[\"text\"]\n",
    "    out = pd.DataFrame({\n",
    "        \"text\": text_series.astype(str),\n",
    "        \"label\": df[\"label\"].astype(int).clip(0, 1)\n",
    "    })\n",
    "    out = drop_empty_dupes(out, \"text\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def standardize_labels(df: pd.DataFrame, label_col: str) -> pd.DataFrame:\n",
    "    # map any common strings -> 0/1  (0 = harmless/ham, 1 = spam)\n",
    "    map_str = {\n",
    "        \"spam\": 1, \"ham\": 0, \"not spam\": 0, \"non-spam\": 0, \"non spam\": 0, \"legit\": 0,\n",
    "        \"harmless\": 0\n",
    "    }\n",
    "    if df[label_col].dtype.kind in \"OUSU\":\n",
    "        df[\"label\"] = df[label_col].astype(str).str.lower().map(map_str)\n",
    "    else:\n",
    "        df[\"label\"] = df[label_col].astype(int).clip(0, 1)\n",
    "    # any unknown strings -> NaN; drop them\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# ---------- Dataset processors ----------\n",
    "def process_email_spam(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    title = cols.get(\"title\")\n",
    "    text  = cols.get(\"text\")\n",
    "    lab   = cols.get(\"type\") or cols.get(\"label\") or cols.get(\"spam\")\n",
    "    if not text:\n",
    "        raise ValueError(\"email_spam.csv needs a 'text' column\")\n",
    "    if not lab:\n",
    "        raise ValueError(\"email_spam.csv needs a 'type'/'label'/'spam' column\")\n",
    "\n",
    "    # combine title + text if title exists\n",
    "    if title:\n",
    "        df[\"text\"] = (df[title].fillna(\"\").astype(str).str.strip() + \" \" +\n",
    "                      df[text].fillna(\"\").astype(str).str.strip()).str.strip()\n",
    "    else:\n",
    "        df[\"text\"] = df[text].fillna(\"\").astype(str)\n",
    "\n",
    "    df = standardize_labels(df, lab)\n",
    "    df = drop_empty_dupes(df, \"text\")\n",
    "    df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "    df = df[df[\"clean_text\"].str.len() > 0]\n",
    "    return to_2col(df)\n",
    "\n",
    "def process_emails(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    text = cols.get(\"text\")\n",
    "    lab  = cols.get(\"spam\") or cols.get(\"label\") or cols.get(\"type\")\n",
    "    if not text or not lab:\n",
    "        raise ValueError(\"emails.csv needs 'text' and 'spam/label/type' columns\")\n",
    "\n",
    "    df[\"text\"] = df[text].fillna(\"\").astype(str)\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"^\\s*subject\\s*:\\s*\", \"\", regex=True)\n",
    "    df = standardize_labels(df, lab)\n",
    "    df = drop_empty_dupes(df, \"text\")\n",
    "    df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "    df = df[df[\"clean_text\"].str.len() > 0]\n",
    "    return to_2col(df)\n",
    "\n",
    "def process_text_spam(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    # Accept capitalized headers from this file\n",
    "    text = cols.get(\"text\") or cols.get(\"message\") or cols.get(\"content\")\n",
    "    lab  = (cols.get(\"label\") or cols.get(\"spam\") or cols.get(\"type\") or\n",
    "            cols.get(\"target\") or cols.get(\"category\"))  # <-- added 'category'\n",
    "\n",
    "    if not text or not lab:\n",
    "        raise ValueError(\"text_spam.csv needs 'text/message/content' AND 'label/spam/type/target/category'\")\n",
    "\n",
    "    df[\"text\"] = df[text].fillna(\"\").astype(str)\n",
    "    df = standardize_labels(df, lab)   # maps 'ham'->0, 'spam'->1 already\n",
    "    df = drop_empty_dupes(df, \"text\")\n",
    "    df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "    df = df[df[\"clean_text\"].str.len() > 0]\n",
    "    return to_2col(df)\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(\"== Preprocessing to 2-column schema: text,label (0 harmless, 1 spam) ==\")\n",
    "    a = process_email_spam(FILES[\"email_spam\"])\n",
    "    b = process_emails(FILES[\"emails\"])\n",
    "    c = process_text_spam(FILES[\"text_spam\"])\n",
    "\n",
    "    a_out = os.path.join(DATA_OUT, \"email_spam.processed.csv\")\n",
    "    b_out = os.path.join(DATA_OUT, \"emails.processed.csv\")\n",
    "    c_out = os.path.join(DATA_OUT, \"text_spam.processed.csv\")\n",
    "    a.to_csv(a_out, index=False); print(f\"Saved: {a_out}  {a.shape}\")\n",
    "    b.to_csv(b_out, index=False); print(f\"Saved: {b_out}  {b.shape}\")\n",
    "    c.to_csv(c_out, index=False); print(f\"Saved: {c_out}  {c.shape}\")\n",
    "\n",
    "    merged = pd.concat([a, b, c], ignore_index=True).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "    m_out = os.path.join(DATA_OUT, \"emails_merged.processed.csv\")\n",
    "    merged.to_csv(m_out, index=False); print(f\"Saved: {m_out}  {merged.shape}\")\n",
    "\n",
    "    # Optional stratified split\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train, test = train_test_split(merged, test_size=0.2, random_state=42, stratify=merged[\"label\"])\n",
    "        train.to_csv(os.path.join(DATA_OUT, \"emails_merged.train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(DATA_OUT, \"emails_merged.test.csv\"), index=False)\n",
    "        print(\"Saved: emails_merged.train.csv / emails_merged.test.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"(split skipped) {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ab39e-e3d7-4eaa-b885-8b0eccebcf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
